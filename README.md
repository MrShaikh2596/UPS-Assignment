# UPS GRI Report 2024 Document Intelligence

This project indexes the UPS GRI Report 2024 PDF into a FAISS vector store and exposes a FastAPI-powered Retrieval-Augmented Generation (RAG) service for querying the document.

---

## 1. Prerequisites & Setup

1. **Create / activate a virtual environment** (optional but recommended).
2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```
3. **Configure environment variables** by creating a `.env` file in the project root containing at least:
   ```bash
   JINA_API_KEY=<your_jina_api_key>
   GROQ_API_KEY=<your_groq_api_key>
   ```
   These keys are required for embedding generation and LLM responses.

---

## 2. Build the FAISS Vector Store

If the FAISS index does not already exist (`faiss_index/`), run:
```bash
python vector_db.py
```
- You will be prompted for a PDF path; press Enter to use the default `UPS Docs\AI Enginner Use Case Document.pdf`.
- The script splits the PDF, generates embeddings with **Jina `jina-embeddings-v4`**, and saves them into `faiss_index/`.

> **Note:** Repeat this step whenever you update the source document.

---

## 3. Test the Retriever (Optional)

To verify that chunks can be retrieved from the FAISS store, run:
```bash
python retriver.py
```
- Enter a query and (optionally) override the `top_k` value when prompted.
- The script will print the retrieved chunks to the console.

---

## 4. Launch the RAG API

Start the FastAPI service with:
```bash
python rag-chat.py
```
- Visit `http://localhost:8000/docs` to access the autogenerated Swagger UI.
- The `/rag` GET endpoint accepts `query` (string) and `top_k` (int, default `5`) parameters and returns both the synthesized answer and the supporting chunks.

---

## 5. Project Structure

```
vector_db.py        # Builds the FAISS index from the UPS PDF
retriver.py         # Loads FAISS and retrieves relevant chunks for a query
rag-chat.py         # FastAPI app exposing /rag endpoint + Swagger UI
requirements.txt    # Python dependencies
faiss_index/        # Persisted FAISS index (created after running vector_db.py)
UPS Docs/           # Default location for the UPS report PDF
```

---

## 6. Troubleshooting

- **Missing FAISS index**: Run `python vector_db.py` before querying.
- **API key errors**: Ensure `.env` contains valid `JINA_API_KEY` and `GROQ_API_KEY` values.

---

## Appendix: Design Decisions

- **Embedding model**: The ideal choice for this project is *ColQwen2*, a state-of-the-art open-source multimodal embedding model tailored for mixed text/tabular inputs such as GRI reports. Due to current environment constraints we use the closest practical alternative, **Jina AI’s `jina-embeddings-v4`**, which still performs well on textual and tabular content.
- **Chat model**: The API employs **`gpt-oss-120b`** via Groq, chosen as an approximate open-source analogue to OpenAI’s flagship GPT-4o model, providing strong reasoning and response quality.
- **Vector database**: **FAISS** is used as an in-memory store—ideal for small-to-medium document collections and fast similarity search without external service dependencies.
- **Chunking strategy**: **LangChain’s `RecursiveCharacterTextSplitter`** with a **1000-character chunk size** and roughly **15–20 % overlap** offers resilient splitting across varied document sections while preserving enough context for retrieval.



## Appendix: Limitations & Future Improvements

- **Open-source baseline**: Cost and time constraints meant relying on open-source models and tooling. Incorporating proprietary or more advanced hosted models could boost accuracy and latency performance.
- **Chunking enhancements**: The current 1000-character recursive chunks with ~15–20 % overlap are a solid default; experimenting with semantic chunking or adaptive sizes may improve retrieval fidelity.
- **Retrieval pipeline**: For this small document, straight semantic similarity search suffices. Introducing re-rankers or multi-stage retrieval could yield better precision on larger or more complex corpora.
- **Async architecture**: The implementation uses synchronous functions. Refactoring to async (e.g., `async def` endpoints, async vector store calls) would better support high-concurrency scenarios.


